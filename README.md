# decentralized-summarizer
[![PyPI version](https://badge.fury.io/py/decentralized-summarizer.svg)](https://badge.fury.io/py/decentralized-summarizer)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Downloads](https://static.pepy.tech/badge/decentralized-summarizer)](https://pepy.tech/project/decentralized-summarizer)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-blue)](https://www.linkedin.com/in/eugene-evstafev-716669181/)


**decentralized-summarizer** is a Python package that turns user input about decentralized protocols (e.g., Polyproto) into clear, structured summaries. It extracts key features and benefits while avoiding technical jargon, making the output accessible to non‑expert audiences. The package leverages pattern matching to guarantee that the generated summaries follow a predefined format.

## Features

- **Simple API** – One function call to generate a summary.
- **LLM‑agnostic** – Uses `ChatLLM7` by default; you can plug any LangChain‑compatible LLM.
- **Formatted output** – Enforces a regex pattern, ensuring consistent structure.
- **No jargon** – Tailored prompts keep language clear and approachable.

## Installation

```bash
pip install decentralized_summarizer
```

## Quick Start

```python
from decentralized_summarizer import decentralized_summarizer

# Simple usage with the default ChatLLM7
summary = decentralized_summarizer(
    user_input="Explain the main advantages of Polyproto in a decentralized network."
)

print(summary)  # => List of formatted summary strings
```

## Function Signature

```python
def decentralized_summarizer(
    user_input: str,
    api_key: Optional[str] = None,
    llm: Optional[BaseChatModel] = None,
) -> List[str]:
```

| Parameter | Type | Description |
|-----------|------|-------------|
| **user_input** | `str` | The raw text you want to summarise. |
| **api_key** | `Optional[str]` | API key for `ChatLLM7`. If omitted, the function reads `LLM7_API_KEY` from the environment or uses a placeholder `"None"` (which works for the free tier). |
| **llm** | `Optional[BaseChatModel]` | A LangChain LLM instance. If not provided, `ChatLLM7` from `langchain_llm7` is used. |

## Using a Custom LLM

You can pass any LangChain‑compatible chat model (OpenAI, Anthropic, Google, etc.):

### OpenAI

```python
from langchain_openai import ChatOpenAI
from decentralized_summarizer import decentralized_summarizer

my_llm = ChatOpenAI(model="gpt-4o-mini")
summary = decentralized_summarizer(
    user_input="What are the security benefits of a decentralized exchange?",
    llm=my_llm,
)
```

### Anthropic

```python
from langchain_anthropic import ChatAnthropic
from decentralized_summarizer import decentralized_summarizer

my_llm = ChatAnthropic(model="claude-3-opus-20240229")
summary = decentralized_summarizer(
    user_input="Describe how decentralised governance works.",
    llm=my_llm,
)
```

### Google Gemini

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from decentralized_summarizer import decentralized_summarizer

my_llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
summary = decentralized_summarizer(
    user_input="Summarise the key points of the Polkadot parachain model.",
    llm=my_llm,
)
```

## API Key & Rate Limits

- The package defaults to `ChatLLM7` from the **langchain_llm7** package.
- The free tier of `ChatLLM7` provides sufficient rate limits for typical usage.
- For higher limits, supply your own API key:
  - **Environment variable:** `export LLM7_API_KEY="your_key"`
  - **Direct argument:** `api_key="your_key"`

Obtain a free API key by registering at: https://token.llm7.io/

## Contributing & Support

- **Issues:** <https://github.com/chigwell/decentralized_summarizer/issues>
- **Pull requests:** Contributions are welcome—please follow standard GitHub workflow.

## Author

**Eugene Evstafev**  
Email: [hi@eugene.plus](mailto:hi@eugene.plus)  
GitHub: [chigwell](https://github.com/chigwell)

---

*The package uses `ChatLLM7` from `langchain_llm7` by default. Feel free to replace it with any other LangChain chat model that matches the `BaseChatModel` interface.*